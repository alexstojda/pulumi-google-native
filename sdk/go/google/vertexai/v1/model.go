// Code generated by the Pulumi SDK Generator DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package v1

import (
	"context"
	"reflect"

	"github.com/pkg/errors"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

type Model struct {
	pulumi.CustomResourceState

	// Immutable. The path to the directory containing the Model artifact and any of its supporting files. Not present for AutoML Models.
	ArtifactUri pulumi.StringPtrOutput `pulumi:"artifactUri"`
	// The specification of the container that is to be used when deploying this Model. The specification is ingested upon ModelService.UploadModel, and all binaries it contains are copied and stored internally by Vertex AI. Not present for AutoML Models.
	ContainerSpec ModelContainerSpecPtrOutput `pulumi:"containerSpec"`
	// Output only. Timestamp when this Model was uploaded into Vertex AI.
	CreateTime pulumi.StringPtrOutput `pulumi:"createTime"`
	// Output only. The pointers to DeployedModels created from this Model. Note that Model could have been deployed to Endpoints in different Locations.
	DeployedModels ModelDeployedModelsArrayOutput `pulumi:"deployedModels"`
	// The description of the Model.
	Description pulumi.StringPtrOutput `pulumi:"description"`
	// Required. The display name of the Model. The name can be up to 128 characters long and can be consist of any UTF-8 characters.
	DisplayName pulumi.StringPtrOutput `pulumi:"displayName"`
	// Customer-managed encryption key spec for a Model. If set, this Model and all sub-resources of this Model will be secured by this key.
	EncryptionSpec ModelEncryptionSpecPtrOutput `pulumi:"encryptionSpec"`
	// Used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
	Etag pulumi.StringPtrOutput `pulumi:"etag"`
	// The labels with user-defined metadata to organize your Models. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
	Labels LabelsPtrOutput `pulumi:"labels"`
	// The location for the resource
	Location pulumi.StringPtrOutput `pulumi:"location"`
	// The resource name of the Model.
	Name pulumi.StringPtrOutput `pulumi:"name"`
	// Output only. If this Model is a copy of another Model, this contains info about the original.
	OriginalModelInfo ModelOriginalModelInfoPtrOutput `pulumi:"originalModelInfo"`
	// The project for the resource
	Project pulumi.StringPtrOutput `pulumi:"project"`
	// Output only. When this Model is deployed, its prediction resources are described by the `prediction_resources` field of the Endpoint.deployed_models object. Because not all Models support all resource configuration types, the configuration types this Model supports are listed here. If no configuration types are listed, the Model cannot be deployed to an Endpoint and does not support online predictions (PredictionService.Predict or PredictionService.Explain). Such a Model can serve predictions by using a BatchPredictionJob, if it has at least one entry each in supported_input_storage_formats and supported_output_storage_formats.
	SupportedDeploymentResourcesTypes ModelSupportedDeploymentResourcesTypesEnumArrayOutput `pulumi:"supportedDeploymentResourcesTypes"`
	// Output only. The formats in which this Model may be exported. If empty, this Model is not available for export.
	SupportedExportFormats ModelSupportedExportFormatsArrayOutput `pulumi:"supportedExportFormats"`
	// Output only. The formats this Model supports in BatchPredictionJob.input_config. If PredictSchemata.instance_schema_uri exists, the instances should be given as per that schema. The possible formats are: * `jsonl` The JSON Lines format, where each instance is a single line. Uses GcsSource. * `csv` The CSV format, where each instance is a single comma-separated line. The first line in the file is the header, containing comma-separated field names. Uses GcsSource. * `tf-record` The TFRecord format, where each instance is a single record in tfrecord syntax. Uses GcsSource. * `tf-record-gzip` Similar to `tf-record`, but the file is gzipped. Uses GcsSource. * `bigquery` Each instance is a single row in BigQuery. Uses BigQuerySource. * `file-list` Each line of the file is the location of an instance to process, uses `gcs_source` field of the InputConfig object. If this Model doesn't support any of these formats it means it cannot be used with a BatchPredictionJob. However, if it has supported_deployment_resources_types, it could serve online predictions by using PredictionService.Predict or PredictionService.Explain. TODO(rsurowka): Give a link describing how OpenAPI schema instances are expressed in JSONL and BigQuery. TODO(rsurowka): Should we provide a schema for TFRecord? Or maybe say that at least for now TFRecord input is not supported via schemata (that would also simplify giving them back as part of predictions). TODO(rsurowka): Define CSV format (decide how much we want to support). E.g. no nesting? Or no arrays, or no nested arrays? E.g. https://json-csv.com/ seems to be able to do pretty advanced conversions, but we may decide to make it relatively simple for now.
	SupportedInputStorageFormats pulumi.StringArrayOutput `pulumi:"supportedInputStorageFormats"`
	// Output only. The formats this Model supports in BatchPredictionJob.output_config. If both PredictSchemata.instance_schema_uri and PredictSchemata.prediction_schema_uri exist, the predictions are returned together with their instances. In other words, the prediction has the original instance data first, followed by the actual prediction content (as per the schema). The possible formats are: * `jsonl` The JSON Lines format, where each prediction is a single line. Uses GcsDestination. * `csv` The CSV format, where each prediction is a single comma-separated line. The first line in the file is the header, containing comma-separated field names. Uses GcsDestination. * `bigquery` Each prediction is a single row in a BigQuery table, uses BigQueryDestination . If this Model doesn't support any of these formats it means it cannot be used with a BatchPredictionJob. However, if it has supported_deployment_resources_types, it could serve online predictions by using PredictionService.Predict or PredictionService.Explain. TODO(rsurowka): Analogous TODOs as for instances field above.
	SupportedOutputStorageFormats pulumi.StringArrayOutput `pulumi:"supportedOutputStorageFormats"`
	// Output only. The resource name of the TrainingPipeline that uploaded this Model, if any.
	TrainingPipeline pulumi.StringPtrOutput `pulumi:"trainingPipeline"`
	// Output only. Timestamp when this Model was most recently updated.
	UpdateTime pulumi.StringPtrOutput `pulumi:"updateTime"`
	// Output only. Timestamp when this version was created.
	VersionCreateTime pulumi.StringPtrOutput `pulumi:"versionCreateTime"`
	// The description of this version.
	VersionDescription pulumi.StringPtrOutput `pulumi:"versionDescription"`
	// Output only. Immutable. The version ID of the model. A new version is committed when a new model version is uploaded or trained under an existing model id. It is an auto-incrementing decimal number in string representation.
	VersionId pulumi.StringPtrOutput `pulumi:"versionId"`
	// Output only. Timestamp when this version was most recently updated.
	VersionUpdateTime pulumi.StringPtrOutput `pulumi:"versionUpdateTime"`
}

// NewModel registers a new resource with the given unique name, arguments, and options.
func NewModel(ctx *pulumi.Context,
	name string, args *ModelArgs, opts ...pulumi.ResourceOption) (*Model, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.ContainerSpec == nil {
		return nil, errors.New("invalid value for required argument 'ContainerSpec'")
	}
	if args.DisplayName == nil {
		return nil, errors.New("invalid value for required argument 'DisplayName'")
	}
	if args.Location == nil {
		return nil, errors.New("invalid value for required argument 'Location'")
	}
	if args.Project == nil {
		return nil, errors.New("invalid value for required argument 'Project'")
	}
	replaceOnChanges := pulumi.ReplaceOnChanges([]string{
		"artifactUri",
		"containerSpec",
		"createTime",
		"deployedModels[*]",
		"encryptionSpec",
		"etag",
		"location",
		"name",
		"originalModelInfo",
		"project",
		"supportedDeploymentResourcesTypes[*]",
		"supportedExportFormats[*]",
		"supportedInputStorageFormats[*]",
		"supportedOutputStorageFormats[*]",
		"trainingPipeline",
		"updateTime",
		"versionCreateTime",
		"versionDescription",
		"versionId",
		"versionUpdateTime",
	})
	opts = append(opts, replaceOnChanges)
	var resource Model
	err := ctx.RegisterResource("google-native:vertexai/v1:Model", name, args, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetModel gets an existing Model resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetModel(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *ModelState, opts ...pulumi.ResourceOption) (*Model, error) {
	var resource Model
	err := ctx.ReadResource("google-native:vertexai/v1:Model", name, id, state, &resource, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering Model resources.
type modelState struct {
}

type ModelState struct {
}

func (ModelState) ElementType() reflect.Type {
	return reflect.TypeOf((*modelState)(nil)).Elem()
}

type modelArgs struct {
	// Immutable. The path to the directory containing the Model artifact and any of its supporting files. Not present for AutoML Models.
	ArtifactUri *string `pulumi:"artifactUri"`
	// The specification of the container that is to be used when deploying this Model. The specification is ingested upon ModelService.UploadModel, and all binaries it contains are copied and stored internally by Vertex AI. Not present for AutoML Models.
	ContainerSpec ModelContainerSpec `pulumi:"containerSpec"`
	// The description of the Model.
	Description *string `pulumi:"description"`
	// Required. The display name of the Model. The name can be up to 128 characters long and can be consist of any UTF-8 characters.
	DisplayName string `pulumi:"displayName"`
	// Customer-managed encryption key spec for a Model. If set, this Model and all sub-resources of this Model will be secured by this key.
	EncryptionSpec *ModelEncryptionSpec `pulumi:"encryptionSpec"`
	// The labels with user-defined metadata to organize your Models. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
	Labels *Labels `pulumi:"labels"`
	// The location for the resource
	Location string `pulumi:"location"`
	// The resource name of the Model.
	Name *string `pulumi:"name"`
	// The project for the resource
	Project string `pulumi:"project"`
	// The description of this version.
	VersionDescription *string `pulumi:"versionDescription"`
}

// The set of arguments for constructing a Model resource.
type ModelArgs struct {
	// Immutable. The path to the directory containing the Model artifact and any of its supporting files. Not present for AutoML Models.
	ArtifactUri pulumi.StringPtrInput
	// The specification of the container that is to be used when deploying this Model. The specification is ingested upon ModelService.UploadModel, and all binaries it contains are copied and stored internally by Vertex AI. Not present for AutoML Models.
	ContainerSpec ModelContainerSpecInput
	// The description of the Model.
	Description pulumi.StringPtrInput
	// Required. The display name of the Model. The name can be up to 128 characters long and can be consist of any UTF-8 characters.
	DisplayName pulumi.StringInput
	// Customer-managed encryption key spec for a Model. If set, this Model and all sub-resources of this Model will be secured by this key.
	EncryptionSpec ModelEncryptionSpecPtrInput
	// The labels with user-defined metadata to organize your Models. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
	Labels LabelsPtrInput
	// The location for the resource
	Location pulumi.StringInput
	// The resource name of the Model.
	Name pulumi.StringPtrInput
	// The project for the resource
	Project pulumi.StringInput
	// The description of this version.
	VersionDescription pulumi.StringPtrInput
}

func (ModelArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*modelArgs)(nil)).Elem()
}

type ModelInput interface {
	pulumi.Input

	ToModelOutput() ModelOutput
	ToModelOutputWithContext(ctx context.Context) ModelOutput
}

func (*Model) ElementType() reflect.Type {
	return reflect.TypeOf((**Model)(nil)).Elem()
}

func (i *Model) ToModelOutput() ModelOutput {
	return i.ToModelOutputWithContext(context.Background())
}

func (i *Model) ToModelOutputWithContext(ctx context.Context) ModelOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ModelOutput)
}

type ModelOutput struct{ *pulumi.OutputState }

func (ModelOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**Model)(nil)).Elem()
}

func (o ModelOutput) ToModelOutput() ModelOutput {
	return o
}

func (o ModelOutput) ToModelOutputWithContext(ctx context.Context) ModelOutput {
	return o
}

// Immutable. The path to the directory containing the Model artifact and any of its supporting files. Not present for AutoML Models.
func (o ModelOutput) ArtifactUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.ArtifactUri }).(pulumi.StringPtrOutput)
}

// The specification of the container that is to be used when deploying this Model. The specification is ingested upon ModelService.UploadModel, and all binaries it contains are copied and stored internally by Vertex AI. Not present for AutoML Models.
func (o ModelOutput) ContainerSpec() ModelContainerSpecPtrOutput {
	return o.ApplyT(func(v *Model) ModelContainerSpecPtrOutput { return v.ContainerSpec }).(ModelContainerSpecPtrOutput)
}

// Output only. Timestamp when this Model was uploaded into Vertex AI.
func (o ModelOutput) CreateTime() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.CreateTime }).(pulumi.StringPtrOutput)
}

// Output only. The pointers to DeployedModels created from this Model. Note that Model could have been deployed to Endpoints in different Locations.
func (o ModelOutput) DeployedModels() ModelDeployedModelsArrayOutput {
	return o.ApplyT(func(v *Model) ModelDeployedModelsArrayOutput { return v.DeployedModels }).(ModelDeployedModelsArrayOutput)
}

// The description of the Model.
func (o ModelOutput) Description() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.Description }).(pulumi.StringPtrOutput)
}

// Required. The display name of the Model. The name can be up to 128 characters long and can be consist of any UTF-8 characters.
func (o ModelOutput) DisplayName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.DisplayName }).(pulumi.StringPtrOutput)
}

// Customer-managed encryption key spec for a Model. If set, this Model and all sub-resources of this Model will be secured by this key.
func (o ModelOutput) EncryptionSpec() ModelEncryptionSpecPtrOutput {
	return o.ApplyT(func(v *Model) ModelEncryptionSpecPtrOutput { return v.EncryptionSpec }).(ModelEncryptionSpecPtrOutput)
}

// Used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
func (o ModelOutput) Etag() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.Etag }).(pulumi.StringPtrOutput)
}

// The labels with user-defined metadata to organize your Models. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
func (o ModelOutput) Labels() LabelsPtrOutput {
	return o.ApplyT(func(v *Model) LabelsPtrOutput { return v.Labels }).(LabelsPtrOutput)
}

// The location for the resource
func (o ModelOutput) Location() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.Location }).(pulumi.StringPtrOutput)
}

// The resource name of the Model.
func (o ModelOutput) Name() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.Name }).(pulumi.StringPtrOutput)
}

// Output only. If this Model is a copy of another Model, this contains info about the original.
func (o ModelOutput) OriginalModelInfo() ModelOriginalModelInfoPtrOutput {
	return o.ApplyT(func(v *Model) ModelOriginalModelInfoPtrOutput { return v.OriginalModelInfo }).(ModelOriginalModelInfoPtrOutput)
}

// The project for the resource
func (o ModelOutput) Project() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.Project }).(pulumi.StringPtrOutput)
}

// Output only. When this Model is deployed, its prediction resources are described by the `prediction_resources` field of the Endpoint.deployed_models object. Because not all Models support all resource configuration types, the configuration types this Model supports are listed here. If no configuration types are listed, the Model cannot be deployed to an Endpoint and does not support online predictions (PredictionService.Predict or PredictionService.Explain). Such a Model can serve predictions by using a BatchPredictionJob, if it has at least one entry each in supported_input_storage_formats and supported_output_storage_formats.
func (o ModelOutput) SupportedDeploymentResourcesTypes() ModelSupportedDeploymentResourcesTypesEnumArrayOutput {
	return o.ApplyT(func(v *Model) ModelSupportedDeploymentResourcesTypesEnumArrayOutput {
		return v.SupportedDeploymentResourcesTypes
	}).(ModelSupportedDeploymentResourcesTypesEnumArrayOutput)
}

// Output only. The formats in which this Model may be exported. If empty, this Model is not available for export.
func (o ModelOutput) SupportedExportFormats() ModelSupportedExportFormatsArrayOutput {
	return o.ApplyT(func(v *Model) ModelSupportedExportFormatsArrayOutput { return v.SupportedExportFormats }).(ModelSupportedExportFormatsArrayOutput)
}

// Output only. The formats this Model supports in BatchPredictionJob.input_config. If PredictSchemata.instance_schema_uri exists, the instances should be given as per that schema. The possible formats are: * `jsonl` The JSON Lines format, where each instance is a single line. Uses GcsSource. * `csv` The CSV format, where each instance is a single comma-separated line. The first line in the file is the header, containing comma-separated field names. Uses GcsSource. * `tf-record` The TFRecord format, where each instance is a single record in tfrecord syntax. Uses GcsSource. * `tf-record-gzip` Similar to `tf-record`, but the file is gzipped. Uses GcsSource. * `bigquery` Each instance is a single row in BigQuery. Uses BigQuerySource. * `file-list` Each line of the file is the location of an instance to process, uses `gcs_source` field of the InputConfig object. If this Model doesn't support any of these formats it means it cannot be used with a BatchPredictionJob. However, if it has supported_deployment_resources_types, it could serve online predictions by using PredictionService.Predict or PredictionService.Explain. TODO(rsurowka): Give a link describing how OpenAPI schema instances are expressed in JSONL and BigQuery. TODO(rsurowka): Should we provide a schema for TFRecord? Or maybe say that at least for now TFRecord input is not supported via schemata (that would also simplify giving them back as part of predictions). TODO(rsurowka): Define CSV format (decide how much we want to support). E.g. no nesting? Or no arrays, or no nested arrays? E.g. https://json-csv.com/ seems to be able to do pretty advanced conversions, but we may decide to make it relatively simple for now.
func (o ModelOutput) SupportedInputStorageFormats() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *Model) pulumi.StringArrayOutput { return v.SupportedInputStorageFormats }).(pulumi.StringArrayOutput)
}

// Output only. The formats this Model supports in BatchPredictionJob.output_config. If both PredictSchemata.instance_schema_uri and PredictSchemata.prediction_schema_uri exist, the predictions are returned together with their instances. In other words, the prediction has the original instance data first, followed by the actual prediction content (as per the schema). The possible formats are: * `jsonl` The JSON Lines format, where each prediction is a single line. Uses GcsDestination. * `csv` The CSV format, where each prediction is a single comma-separated line. The first line in the file is the header, containing comma-separated field names. Uses GcsDestination. * `bigquery` Each prediction is a single row in a BigQuery table, uses BigQueryDestination . If this Model doesn't support any of these formats it means it cannot be used with a BatchPredictionJob. However, if it has supported_deployment_resources_types, it could serve online predictions by using PredictionService.Predict or PredictionService.Explain. TODO(rsurowka): Analogous TODOs as for instances field above.
func (o ModelOutput) SupportedOutputStorageFormats() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *Model) pulumi.StringArrayOutput { return v.SupportedOutputStorageFormats }).(pulumi.StringArrayOutput)
}

// Output only. The resource name of the TrainingPipeline that uploaded this Model, if any.
func (o ModelOutput) TrainingPipeline() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.TrainingPipeline }).(pulumi.StringPtrOutput)
}

// Output only. Timestamp when this Model was most recently updated.
func (o ModelOutput) UpdateTime() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.UpdateTime }).(pulumi.StringPtrOutput)
}

// Output only. Timestamp when this version was created.
func (o ModelOutput) VersionCreateTime() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.VersionCreateTime }).(pulumi.StringPtrOutput)
}

// The description of this version.
func (o ModelOutput) VersionDescription() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.VersionDescription }).(pulumi.StringPtrOutput)
}

// Output only. Immutable. The version ID of the model. A new version is committed when a new model version is uploaded or trained under an existing model id. It is an auto-incrementing decimal number in string representation.
func (o ModelOutput) VersionId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.VersionId }).(pulumi.StringPtrOutput)
}

// Output only. Timestamp when this version was most recently updated.
func (o ModelOutput) VersionUpdateTime() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Model) pulumi.StringPtrOutput { return v.VersionUpdateTime }).(pulumi.StringPtrOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*ModelInput)(nil)).Elem(), &Model{})
	pulumi.RegisterOutputType(ModelOutput{})
}
